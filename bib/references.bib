@article{wadhawan2020deep,
title = { Deep learning-based sign language recognition system for static signs },
journal = { Neural computing and applications },
volume = {32},
pages = {7957--7968},
year = {2020},
note = { Deep learning-based sign language recognition },
issn = { 1433-3058 },
doi = { 10.1007/s00521-019-04691-y },
url = { https://doi.org/10.1007/s00521-019-04691-y },
author = { Wadhawan, Ankita and Kumar, Parteek },
keywords = { Sign language,Data acquisition, Convolutional neural network, Max-pooling, Softmax, Optimizer
},
abstract = { Sign language for communication is efficacious for humans, and vital research is in progress in computer vision systems. The earliest work in Indian Sign Language (ISL) recognition considers the recognition of significant differentiable hand signs and therefore often selecting a few signs from the ISL for recognition. This paper deals with robust modeling of static signs in the context of sign language recognition using deep learning-based convolutional neural networks (CNN). In this research, total 35,000 sign images of 100 static signs are collected from different users. The efficiency of the proposed system is evaluated on approximately 50 CNN models. The results are also evaluated on the basis of different optimizers, and it has been observed that the proposed approach has achieved the highest training accuracy of 99.72% and 99.90% on colored and grayscale images, respectively. The performance of the proposed system has also been evaluated on the basis of precision, recall and F-score. The system also demonstrates its effectiveness over the earlier works in which only a few hand signs are considered for recognition. }
}

@article{KUMAR201721,
title = {A multimodal framework for sensor based sign language recognition},
journal = {Neurocomputing},
volume = {259},
pages = {21-38},
year = {2017},
note = {Multimodal Media Data Understanding and Analytics},
issn = {0925-2312},
doi = {https://doi.org/10.1016/j.neucom.2016.08.132},
url = {https://www.sciencedirect.com/science/article/pii/S092523121730262X},
author = {Pradeep Kumar and Himaanshu Gauba and Partha {Pratim Roy} and Debi {Prosad Dogra}},
keywords = {Sign language recognition, Gesture recognition, Multimodal framework, Sensor fusion},
abstract = {In this paper, we propose a novel multimodal framework for isolated Sign Language Recognition (SLR) using sensor devices. Microsoft Kinect and Leap motion sensors are used in our framework to capture finger and palm positions from two different views during gesture. One sensor (Leap Motion) is kept below the hand(s) while the other (Kinect) is placed in front of the signer for capturing horizontal and vertical movement of fingers during sign gestures. A set of features is next extracted from the raw data captured with both sensors. Recognition is performed separately by Hidden Markov Model (HMM) and Bidirectional Long Short-Term Memory Neural Network (BLSTM-NN) based sequential classifiers. In the next phase, results are combined to boost-up the recognition performance. The framework has been tested on a dataset of 7500 Indian Sign Language (ISL) gestures comprised with 50 different sign-words. Our dataset includes single as well as double handed gestures. It has been observed that, accuracies can be improved if data from both sensors are fused as compared to single sensor-based recognition. We have recorded improvements of 2.26% (single hand) and 0.91% (both hands) using HMM and 2.88% (single hand) and 1.67% (both hands) using BLSTM-NN classifiers. Overall accuracies of 97.85% and 94.55% have been recorded by combining HMM and BLSTM-NN for single hand and double handed signs.}
}

@article{KUMAR20171,
title = {Coupled HMM-based multi-sensor data fusion for sign language recognition},
journal = {Pattern Recognition Letters},
volume = {86},
pages = {1-8},
year = {2017},
issn = {0167-8655},
doi = {https://doi.org/10.1016/j.patrec.2016.12.004},
url = {https://www.sciencedirect.com/science/article/pii/S0167865516303518},
author = {Pradeep Kumar and Himaanshu Gauba and Partha Pratim Roy and Debi Prosad Dogra},
keywords = {Sign language recognition, Depth sensors, Hidden Markov model (Coupled HMM, HMM), Bayesian classification},
abstract = {Recent development of low cost depth sensors such as Leap motion controller and Microsoft kinect sensor has opened up new opportunities for Human-Computer-Interaction (HCI). In this paper, we propose a novel multi-sensor fusion framework for Sign Language Recognition (SLR) using Coupled Hidden Markov Model (CHMM). CHMM provides interaction in state-space instead of observation states as used in classical HMM that fails to model correlation between inter-modal dependencies. The framework has been used to recognize dynamic isolated sign gestures performed by hearing impaired persons. The dataset has been tested using existing data fusion approaches. The best recognition accuracy has been achieved as high as 90.80% with CHMM. Our CHMM-based approach shows improvement in recognition performance over popular existing data fusion techniques.}
}

@ARTICLE{8598757,
  author={Cui, Runpeng and Liu, Hu and Zhang, Changshui},
  journal={IEEE Transactions on Multimedia}, 
  title={A Deep Neural Framework for Continuous Sign Language Recognition by Iterative Training}, 
  year={2019},
  volume={21},
  number={7},
  pages={1880-1891},
  keywords={Feature extraction;Training;Videos;Hidden Markov models;Convolutional neural networks;Gesture recognition;Continuous sign language recognition;sequence learning;iterative training;multimodal fusion},
  doi={10.1109/TMM.2018.2889563}}


@INPROCEEDINGS{8580268,
  author={Rosero-Montalvo, Paul D. and Godoy-Trujillo, Pamela and Flores-Bosmediano, Edison and Carrascal-García, Jorge and Otero-Potosi, Santiago and Benitez-Pereira, Henry and Peluffo-Ordóñez, Diego H.},
  booktitle={2018 IEEE Third Ecuador Technical Chapters Meeting (ETCM)}, 
  title={Sign Language Recognition Based on Intelligent Glove Using Machine Learning Techniques}, 
  year={2018},
  volume={},
  number={},
  pages={1-5},
  keywords={Training;Assistive technology;Gesture recognition;Prototypes;Machine learning algorithms;Intelligent sensors;prototype selection;knn;sign language;intelligent glove},
  doi={10.1109/ETCM.2018.8580268}}

@ARTICLE{9187644,
  author={Gurbuz, Sevgi Z. and Gurbuz, Ali Cafer and Malaia, Evie A. and Griffin, Darrin J. and Crawford, Chris S. and Rahman, Mohammad Mahbubur and Kurtoglu, Emre and Aksu, Ridvan and Macks, Trevor and Mdrafi, Robiulhossain},
  journal={IEEE Sensors Journal}, 
  title={American Sign Language Recognition Using RF Sensing}, 
  year={2021},
  volume={21},
  number={3},
  pages={3763-3775},
  keywords={Sensors;Radio frequency;Auditory system;Gesture recognition;Assistive technology;Linguistics;Electronic mail;American sign language;RF sensing;radar;micro-Doppler;machine learning},
  doi={10.1109/JSEN.2020.3022376}}

@INPROCEEDINGS{7424365,
  author={Savur, Celal and Sahin, Ferat},
  booktitle={2015 IEEE 14th International Conference on Machine Learning and Applications (ICMLA)}, 
  title={Real-Time American Sign Language Recognition System Using Surface EMG Signal}, 
  year={2015},
  volume={},
  number={},
  pages={497-502},
  keywords={Assistive technology;Gesture recognition;Support vector machines;Mathematical mod-el;Electromyography;Feature extraction;Sensors;Real-time EMG classification;Support Vector Ma-chine;American Sign Language (ASL)},
  doi={10.1109/ICMLA.2015.212}}

@INPROCEEDINGS{9943235,
  author={Li, Lanxi and Liu, Da and Shen, Chenlin and Sun, Jing},
  booktitle={2022 International Conference on Machine Learning and Intelligent Systems Engineering (MLISE)}, 
  title={American Sign Language Recognition Based on Machine Learning and Neural Network}, 
  year={2022},
  volume={},
  number={},
  pages={452-457},
  keywords={Support vector machines;Machine learning algorithms;Neural networks;Machine learning;Gesture recognition;Assistive technologies;Prediction algorithms;Sign Language Recognition;Manifold;Machine learning;CNN;Dimension reduction},
  doi={10.1109/MLISE57402.2022.00096}}





@article{singh2023reliable,
  title={A reliable and efficient machine learning pipeline for american sign lan-guage gesture recognition using EMG sensors},
  author={Singh, Shashank Kumar and Chaturvedi, Amrita},
  journal={Multimedia Tools and Applications},
  volume={82},
  number={15},
  pages={23833--23871},
  year={2023},
  publisher={Springer}
}

@INPROCEEDINGS{7033173,
  author={Chuan, Ching-Hua and Regina, Eric and Guardino, Caroline},
  booktitle={2014 13th International Conference on Machine Learning and Applications}, 
  title={American Sign Language Recognition Using Leap Motion Sensor}, 
  year={2014},
  volume={},
  number={},
  pages={541-544},
  keywords={Thumb;Vectors;Support vector machines;Assistive technology;Gesture recognition;Accuracy;American Sign Language; 3D Leap Motion sensor; k-nearest neighbor; support vector machine; deaf education},
  doi={10.1109/ICMLA.2014.110}}

